{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d2a9895dd41d1e76f2393525a460ffa9a0e48f44"
   },
   "source": [
    "In this notebook, we preprocessed the data and feed the data to gradient boosting tree models, and got 1.39 on public leaderboard.\n",
    "\n",
    "the workflow is as follows:\n",
    "1. **Data preprocessing**. The purpose of data preprocessing is to achieve higher time/space efficiency. What we did includes round, constant features removal, duplicate features removal, insignificant features removal, etc. The key here is to ensure the preprocessing shall not hurt the accuracy.\n",
    "2. **Feature transform**. The purpose of feature transform is to help the models to better grasp the information in the data, and fight overfitting. What we did includes dropping features which \"live\" on different distributions on training/testing set, adding statistical features, adding low-dimensional representation as features. \n",
    "3. **Modeling**.  We used 2 models: xgboost and lightgbm. We averaged the 2 models for the final prediction.\n",
    "\n",
    "Stay tuned, more update will come. \n",
    "\n",
    "references:\n",
    "* [Distribution of Test vs. Training data](https://www.kaggle.com/nanomathias/distribution-of-test-vs-training-data)\n",
    "* [Ensemble of LGBM and XGB](https://www.kaggle.com/lightsalsa/ensemble-of-lgbm-and-xgb)\n",
    "* [predict house prices-model tuning & ensemble](https://www.kaggle.com/alexpengxiao/predict-house-prices-model-tuning-ensemble)\n",
    "* [Stacked Regressions : Top 4% on LeaderBoard](https://www.kaggle.com/alexpengxiao/predict-house-prices-model-tuning-ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "38cf74c8cb535c9a9a74f025ea963528a552ecc0"
   },
   "source": [
    "**step 1**: load train & test data, drop duplicate columns, round the features to NUM_OF_DECIMALS decimals. here NUM_OF_DECIMALS is a experience value which can be tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4459, 4730)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "train = pd.read_csv('./train/train.csv')\n",
    "test = pd.read_csv('./test/test.csv')\n",
    "test_ID = test['ID']\n",
    "y_train = train['target']\n",
    "y_train = np.log1p(y_train)\n",
    "train.drop(\"ID\", axis = 1, inplace = True)\n",
    "train.drop(\"target\", axis = 1, inplace = True)\n",
    "test.drop(\"ID\", axis = 1, inplace = True)\n",
    "cols_with_onlyone_val = train.columns[train.nunique() == 1]\n",
    "train.drop(cols_with_onlyone_val.values, axis=1, inplace=True)\n",
    "test.drop(cols_with_onlyone_val.values, axis=1, inplace=True)\n",
    "NUM_OF_DECIMALS = 32\n",
    "train = train.round(NUM_OF_DECIMALS)\n",
    "test = test.round(NUM_OF_DECIMALS)\n",
    "colsToRemove = []\n",
    "columns = train.columns\n",
    "for i in range(len(columns)-1):\n",
    "    v = train[columns[i]].values\n",
    "    dupCols = []\n",
    "    for j in range(i + 1,len(columns)):\n",
    "        if np.array_equal(v, train[columns[j]].values):\n",
    "            colsToRemove.append(columns[j])\n",
    "train.drop(colsToRemove, axis=1, inplace=True) \n",
    "test.drop(colsToRemove, axis=1, inplace=True) \n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0974c5c28779c696c15344b86e11dee097ffc48a"
   },
   "source": [
    "**step 2**: Select features by importance. here we used a weak RandomForestRegressor to get the feature importance. here we select top NUM_OF_FEATURES important features. NUM_OF_FEATURES here is a hyper parameter that can be tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "3fc53e695d5f40074447dc85ea94a196e899a292"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10335278212\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4459, 1000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn import ensemble\n",
    "NUM_OF_FEATURES = 1000\n",
    "def rmsle(y, pred):\n",
    "    return np.sqrt(np.mean(np.power(np.log1p(y)-np.log1p(pred), 2)))\n",
    "\n",
    "x1, x2, y1, y2 = model_selection.train_test_split(\n",
    "    train, y_train.values, test_size=0.20, random_state=5)\n",
    "model = ensemble.RandomForestRegressor(n_jobs=-1, random_state=7)\n",
    "model.fit(x1, y1)\n",
    "print(rmsle(y2, model.predict(x2)))\n",
    "\n",
    "col = pd.DataFrame({'importance': model.feature_importances_, 'feature': train.columns}).sort_values(\n",
    "    by=['importance'], ascending=[False])[:NUM_OF_FEATURES]['feature'].values\n",
    "train = train[col]\n",
    "test = test[col]\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "af1d686dcafd355df61cfb461d74bc2a864acca0"
   },
   "source": [
    "**step 3**: we try to test the training data and testing data with Kolmogorov-Smirnov test. This is a two-sided test for the null hypothesis that whether 2 independent samples are drawn from the same continuous distribution([see more](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.ks_2samp.html)). If a feature has different distributions in training set than in testing set, we should remove this feature since what we learned during training cannot generalize. THRESHOLD_P_VALUE and THRESHOLD_STATISTIC are hyper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "99daf8d41b26ce2a402dd0a136c5547b727eb124"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4459, 982)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "THRESHOLD_P_VALUE = 0.01 #need tuned\n",
    "THRESHOLD_STATISTIC = 0.2 #need tuned\n",
    "diff_cols = []\n",
    "for col in train.columns:\n",
    "    statistic, pvalue = ks_2samp(train[col].values, test[col].values)\n",
    "    if pvalue <= THRESHOLD_P_VALUE and np.abs(statistic) > THRESHOLD_STATISTIC:\n",
    "        diff_cols.append(col)\n",
    "for col in diff_cols:\n",
    "    if col in train.columns:\n",
    "        train.drop(col, axis=1, inplace=True)\n",
    "        test.drop(col, axis=1, inplace=True)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "be30c9c3bca91f0eadcd1ed349c0e1c5538a3702"
   },
   "source": [
    "**step 4**: We add some additional statistical features to the original features. Moreover, we also added low-dimensional representations as features. NUM_OF_COM is hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "f294c53ebcf38aa7017a73230775d356a514c919"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% |                                                                        |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.any(np.isnan(total_df) False\n",
      "np.all(np.isfinite(total_df) True\n",
      "total_df.columns: RangeIndex(start=0, stop=1091, step=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4459, 1191)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import random_projection\n",
    "from sklearn import random_projection\n",
    "from sklearn.preprocessing import scale, MinMaxScaler\n",
    "import gc\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "import time\n",
    "import progressbar\n",
    "\n",
    "ntrain = len(train)\n",
    "ntest = len(test)\n",
    "tmp = pd.concat([train,test])#RandomProjection\n",
    "weight = ((train != 0).sum()/len(train)).values\n",
    "\n",
    "tmp_train = train[train!=0]\n",
    "tmp_test = test[test!=0]\n",
    "train[\"weight_count\"] = (tmp_train*weight).sum(axis=1)\n",
    "test[\"weight_count\"] = (tmp_test*weight).sum(axis=1)\n",
    "train[\"count_not0\"] = (train != 0).sum(axis=1)\n",
    "test[\"count_not0\"] = (test != 0).sum(axis=1)\n",
    "train[\"sum\"] = train.sum(axis=1)\n",
    "test[\"sum\"] = test.sum(axis=1)\n",
    "train[\"var\"] = tmp_train.var(axis=1)\n",
    "test[\"var\"] = tmp_test.var(axis=1)\n",
    "train[\"median\"] = tmp_train.median(axis=1)\n",
    "test[\"median\"] = tmp_test.median(axis=1)\n",
    "train[\"mean\"] = tmp_train.mean(axis=1)\n",
    "test[\"mean\"] = tmp_test.mean(axis=1)\n",
    "train[\"std\"] = tmp_train.std(axis=1)\n",
    "test[\"std\"] = tmp_test.std(axis=1)\n",
    "train[\"max\"] = tmp_train.max(axis=1)\n",
    "test[\"max\"] = tmp_test.max(axis=1)\n",
    "train[\"min\"] = tmp_train.min(axis=1)\n",
    "test[\"min\"] = tmp_test.min(axis=1)\n",
    "del(tmp_train)\n",
    "del(tmp_test)\n",
    "\n",
    "# train data is valid , test data has nan and infinite\n",
    "tmp = pd.DataFrame(np.nan_to_num(tmp))\n",
    "# Go through the columns one at a time (can't do it all at once for this dataset)\n",
    "total_df = deepcopy(tmp)      \n",
    "print('np.any(np.isnan(total_df)', np.any(np.isnan(total_df)))\n",
    "print('np.all(np.isfinite(total_df)', np.all(np.isfinite(total_df)))\n",
    "\n",
    "p = progressbar.ProgressBar()\n",
    "p.start()\n",
    "\n",
    "# Mean-variance scale all columns excluding 0-values'\n",
    "print('total_df.columns:',total_df.columns) \n",
    "columnsCount = len(total_df.columns)\n",
    "for col in total_df.columns:    \n",
    "    p.update(col/columnsCount * 100)\n",
    "\n",
    "    # Detect outliers in this column\n",
    "    data = total_df[col].values\n",
    "    data_mean, data_std = np.mean(data), np.std(data)\n",
    "    cut_off = data_std * 3\n",
    "    lower, upper = data_mean - cut_off, data_mean + cut_off\n",
    "    outliers = [x for x in data if x < lower or x > upper]\n",
    "    \n",
    "    # If there are crazy high values, do a log-transform\n",
    "    if len(outliers) > 0:\n",
    "        non_zero_idx = data != 0\n",
    "        total_df.loc[non_zero_idx, col] = np.log(data[non_zero_idx])\n",
    "    \n",
    "    # Scale non-zero column values\n",
    "    nonzero_rows = total_df[col] != 0\n",
    "    if  np.isfinite(total_df.loc[nonzero_rows, col]).all():\n",
    "        total_df.loc[nonzero_rows, col] = scale(total_df.loc[nonzero_rows, col])\n",
    "        if  np.isfinite(total_df[col]).all():\n",
    "            # Scale all column values\n",
    "            total_df[col] = scale(total_df[col])\n",
    "    gc.collect()\n",
    "    \n",
    "p.finish()\n",
    "\n",
    "NUM_OF_COM = 100 #need tuned\n",
    "transformer = random_projection.SparseRandomProjection(n_components = NUM_OF_COM)\n",
    "RP = transformer.fit_transform(tmp)\n",
    "rp = pd.DataFrame(RP)\n",
    "columns = [\"RandomProjection{}\".format(i) for i in range(NUM_OF_COM)]\n",
    "rp.columns = columns\n",
    "\n",
    "rp_train = rp[:ntrain]\n",
    "rp_test = rp[ntrain:]\n",
    "rp_test.index = test.index\n",
    "\n",
    "#concat RandomProjection and raw data\n",
    "train = pd.concat([train,rp_train],axis=1)\n",
    "test = pd.concat([test,rp_test],axis=1)\n",
    "\n",
    "del(rp_train)\n",
    "del(rp_test)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8ecc907903eb1a867d3b7ce335f4186dd6b6f361"
   },
   "source": [
    "**step 5**: Define cross-validation methods and models. xgboost and lightgbm are used as base models. the hyper parameters are already tuned by grid search, here we use them directly. NUM_FOLDS can be treat as hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "e8cff789a25ebbd4031926767a3f3715934ef0d0"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "#define evaluation method for a given model. we use k-fold cross validation on the training set. \n",
    "#the loss function is root mean square logarithm error between target and prediction\n",
    "#note: train and y_train are feeded as global variables\n",
    "NUM_FOLDS = 8 #need tuned\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(NUM_FOLDS, shuffle=True, random_state=42).get_n_splits(train.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)\n",
    "model_xgb = xgb.XGBRegressor(colsample_bytree=0.054, colsample_bylevel =0.5, \n",
    "                             gamma=1.45, learning_rate=0.02, max_depth=22, \n",
    "                             objective='reg:linear',booster='gbtree',\n",
    "                             min_child_weight=57, n_estimators=1000, reg_alpha=0, \n",
    "                             reg_lambda = 0,eval_metric = 'rmse', subsample=0.67, \n",
    "                             silent=1, n_jobs = -1, early_stopping_rounds = 14,\n",
    "                             random_state =7, nthread = -1)\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=144,\n",
    "                              learning_rate=0.005, n_estimators=720, max_depth=13,\n",
    "                              metric='rmse',is_training_metric=True,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,verbose=-1,\n",
    "                              bagging_freq = 5, feature_fraction = 0.9)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "046ae60b016cc480725714dd6f09ae687b75219e"
   },
   "source": [
    "**step 6**: average the two base models and submit the final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "fa1fedd6673d5781b2ed735382d898e21cdd952a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgboost score: 1.3693 (0.0399)\n",
      "\n",
      "LGBM score: 1.3551 (0.0409)\n",
      "\n",
      "averaged score: 1.3531 (0.0408)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ensemble method: model averaging\n",
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    # we define clones of the original models to fit the data in\n",
    "    # the reason of clone is avoiding affect the original base models\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]  \n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    #Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([ model.predict(X) for model in self.models_ ])\n",
    "        return np.mean(predictions, axis=1)\n",
    "\n",
    "#cross validation   \n",
    "score = rmsle_cv(model_xgb)\n",
    "print(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "score = rmsle_cv(model_lgb)\n",
    "print(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))\n",
    "averaged_models = AveragingModels(models = (model_xgb, model_lgb))\n",
    "score = rmsle_cv(averaged_models)\n",
    "print(\"averaged score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))\n",
    "#predict and submit \n",
    "averaged_models.fit(train.values, y_train)\n",
    "pred = np.expm1(averaged_models.predict(test.values))\n",
    "ensemble = pred\n",
    "sub = pd.DataFrame()\n",
    "sub['ID'] = test_ID\n",
    "sub['target'] = ensemble\n",
    "sub.to_csv('submission.csv',index=False)\n",
    "\n",
    "#Xgboost score: 1.3582 (0.0640)\n",
    "#LGBM score: 1.3437 (0.0519)\n",
    "#averaged score: 1.3431 (0.0586)\n",
    "\n",
    "#Xgboost score: 1.3566 (0.0525)\n",
    "#LGBM score: 1.3477 (0.0497)\n",
    "#averaged score: 1.3438 (0.0516)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
