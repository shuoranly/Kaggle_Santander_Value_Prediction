{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "56f3c3499260fe04c9d0c6535f59ea3320b8c987",
    "collapsed": true
   },
   "source": [
    "> Please go through Giba's post and kernel  to underrstand what this leak is all about\n",
    "> https://www.kaggle.com/titericz/the-property-by-giba (kernel)\n",
    "> https://www.kaggle.com/c/santander-value-prediction-challenge/discussion/61329 (post)\n",
    "> \n",
    "> Also, go through this Jiazhen's kernel which finds more columns to exploit leak\n",
    "> https://www.kaggle.com/johnfarrell/giba-s-property-extended-result\n",
    "> \n",
    "> I just exploit data property in brute force way and then fill in remaining by row non zero means! This should bring everyone on level-playing field.\n",
    "> \n",
    "> **Let the competition begin! :D**\n",
    "\n",
    "### Just some small modifications from [original baseline](https://www.kaggle.com/tezdhar/breaking-lb-fresh-start)~\n",
    "- The leak rows are calculated separately on train/test set\n",
    "- Calculated the leaky values, correctness, for each lag\n",
    "- Hope this can help to do some *lag_selection*\n",
    "\n",
    "### Update leak process codes to Dmitry Frumkin's *fast* [version](https://www.kaggle.com/dfrumkin/a-simple-way-to-use-giba-s-features-v2)\n",
    "- The result of Dmitry's original function and result of Hasan's function seem slightly different\n",
    "- Modified to make the output consistent with Hasan's function (Seems better score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from scipy.stats import mode, skew, kurtosis, entropy\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import dask.dataframe as dd\n",
    "from dask.multiprocessing import get\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "tqdm.pandas(tqdm_notebook)\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dc37a766646b5993cef0bc87ad6882728dd20cb2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "transact_cols = [f for f in train.columns if f not in [\"ID\", \"target\"]]\n",
    "y = np.log1p(train[\"target\"]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9951f375af0d5a753031352e04a85a5a12a93e18",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test[\"target\"] = train[\"target\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6dcfc4df1340c38bfeac43fd4d19ba2763b3b916"
   },
   "source": [
    "We take time series columns from [here](https://www.kaggle.com/johnfarrell/giba-s-property-extended-result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = ['f190486d6', '58e2e02e6', 'eeb9cd3aa', '9fd594eec', '6eef030c1', '15ace8c9f', \n",
    "        'fb0f5dbfe', '58e056e12', '20aa07010', '024c577b9', 'd6bb78916', 'b43a7cfd5', \n",
    "        '58232a6fb', '1702b5bf0', '324921c7b', '62e59a501', '2ec5b290f', '241f0f867', \n",
    "        'fb49e4212', '66ace2992', 'f74e8f13d', '5c6487af1', '963a49cdc', '26fc93eb7', \n",
    "        '1931ccfdd', '703885424', '70feb1494', '491b9ee45', '23310aa6f', 'e176a204a', \n",
    "        '6619d81fc', '1db387535', \n",
    "        'fc99f9426', '91f701ba2', '0572565c2', '190db8488', 'adb64ff71', 'c47340d97', 'c5a231d81', '0ff32eb98'\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2664b88d70c76fde0df57ca6f22f75997f72cb00",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from: https://www.kaggle.com/dfrumkin/a-simple-way-to-use-giba-s-features-v2\n",
    "def _get_leak(df, cols, lag=0):\n",
    "    d1 = df[cols[:-lag-2]].apply(tuple, axis=1).to_frame().rename(columns={0: 'key'})\n",
    "    d2 = df[cols[lag+2:]].apply(tuple, axis=1).to_frame().rename(columns={0: 'key'})\n",
    "    d2['pred'] = df[cols[lag]]\n",
    "    #d2 = d2[d2.pred != 0] ### to make output consistent with Hasan's function\n",
    "    d3 = d2[~d2.duplicated(['key'], keep=False)]\n",
    "    return d1.merge(d3, how='left', on='key').pred.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d61c75092518f50a879e9e3d5883ab752f73912b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compiled_leak_result():\n",
    "    \n",
    "    max_nlags = len(cols) - 2\n",
    "    train_leak = train[[\"ID\", \"target\"] + cols]\n",
    "    train_leak[\"compiled_leak\"] = 0\n",
    "    train_leak[\"nonzero_mean\"] = train[transact_cols].apply(\n",
    "        lambda x: np.expm1(np.log1p(x[x!=0]).mean()), axis=1\n",
    "    )\n",
    "    \n",
    "    scores = []\n",
    "    leaky_value_counts = []\n",
    "    leaky_value_corrects = []\n",
    "    leaky_cols = []\n",
    "    \n",
    "    for i in range(max_nlags):\n",
    "        c = \"leaked_target_\"+str(i)\n",
    "        \n",
    "        print('Processing lag', i)\n",
    "        train_leak[c] = _get_leak(train_leak, cols, i)\n",
    "        \n",
    "        leaky_cols.append(c)\n",
    "        train_leak = train.join(\n",
    "            train_leak.set_index(\"ID\")[leaky_cols+[\"compiled_leak\", \"nonzero_mean\"]], \n",
    "            on=\"ID\", how=\"left\"\n",
    "        )[[\"ID\", \"target\"] + cols + leaky_cols+[\"compiled_leak\", \"nonzero_mean\"]]\n",
    "        zeroleak = train_leak[\"compiled_leak\"]==0\n",
    "        train_leak.loc[zeroleak, \"compiled_leak\"] = train_leak.loc[zeroleak, c]\n",
    "        leaky_value_counts.append(sum(train_leak[\"compiled_leak\"] > 0))\n",
    "        _correct_counts = sum(train_leak[\"compiled_leak\"]==train_leak[\"target\"])\n",
    "        leaky_value_corrects.append(_correct_counts/leaky_value_counts[-1])\n",
    "        print(\"Leak values found in train\", leaky_value_counts[-1])\n",
    "        print(\n",
    "            \"% of correct leaks values in train \", \n",
    "            leaky_value_corrects[-1]\n",
    "        )\n",
    "        tmp = train_leak.copy()\n",
    "        tmp.loc[zeroleak, \"compiled_leak\"] = tmp.loc[zeroleak, \"nonzero_mean\"]\n",
    "        scores.append(np.sqrt(mean_squared_error(y, np.log1p(tmp[\"compiled_leak\"]).fillna(14.49))))\n",
    "        print(\n",
    "            'Score (filled with nonzero mean)', \n",
    "            scores[-1]\n",
    "        )\n",
    "    result = dict(\n",
    "        score=scores, \n",
    "        leaky_count=leaky_value_counts,\n",
    "        leaky_correct=leaky_value_corrects,\n",
    "    )\n",
    "    return train_leak, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "68f3fde6e5e9d274a4de6ef0975bbb4b682d5e85",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_leak, result = compiled_leak_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8d904afe6c581aa250592432402977b1ab2b3ede",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame.from_dict(result, orient='columns')\n",
    "result.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "629ec0c6ee122fd82bf40f09a55a3f6f8f6c7fdf",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result.to_csv('train_leaky_stat.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "42bfb7c9b41f687c189229f83cc6c8c7fd100536",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_leak.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "15e49a1e6f3d95150a4114edca7d3ae910bc6ee3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_score = np.min(result['score'])\n",
    "best_lag = np.argmin(result['score'])\n",
    "print('best_score', best_score, '\\nbest_lag', best_lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9dd516686724d756329a5f42c7c53b688c55aceb",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rewrite_compiled_leak(leak_df, lag):\n",
    "    leak_df[\"compiled_leak\"] = 0\n",
    "    for i in range(lag):\n",
    "        c = \"leaked_target_\"+str(i)\n",
    "        zeroleak = leak_df[\"compiled_leak\"]==0\n",
    "        leak_df.loc[zeroleak, \"compiled_leak\"] = leak_df.loc[zeroleak, c]\n",
    "    return leak_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c4982c4cd26a0c84ec34402fd7816f49c630e191",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "leaky_cols = [c for c in train_leak.columns if 'leaked_target_' in c]\n",
    "train_leak = rewrite_compiled_leak(train_leak, best_lag)\n",
    "train_leak[['ID']+leaky_cols+['compiled_leak']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a8902d9458668d79eef27c6761a3c6fd43578824",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_res = train_leak[leaky_cols+['compiled_leak']].replace(0.0, np.nan)\n",
    "train_res.to_csv('train_leak.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8ce80d9f0bf6a44471921328a6fd2c274821485d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compiled_leak_result_test(max_nlags):\n",
    "    test_leak = test[[\"ID\", \"target\"] + cols]\n",
    "    test_leak[\"compiled_leak\"] = 0\n",
    "    test_leak[\"nonzero_mean\"] = test[transact_cols].apply(\n",
    "        lambda x: np.expm1(np.log1p(x[x!=0]).mean()), axis=1\n",
    "    )\n",
    "    \n",
    "    scores = []\n",
    "    leaky_value_counts = []\n",
    "    # leaky_value_corrects = []\n",
    "    leaky_cols = []\n",
    "    \n",
    "    for i in range(max_nlags):\n",
    "        c = \"leaked_target_\"+str(i)\n",
    "        \n",
    "        print('Processing lag', i)\n",
    "        test_leak[c] = _get_leak(test_leak, cols, i)\n",
    "        \n",
    "        leaky_cols.append(c)\n",
    "        test_leak = test.join(\n",
    "            test_leak.set_index(\"ID\")[leaky_cols+[\"compiled_leak\", \"nonzero_mean\"]], \n",
    "            on=\"ID\", how=\"left\"\n",
    "        )[[\"ID\", \"target\"] + cols + leaky_cols+[\"compiled_leak\", \"nonzero_mean\"]]\n",
    "        zeroleak = test_leak[\"compiled_leak\"]==0\n",
    "        test_leak.loc[zeroleak, \"compiled_leak\"] = test_leak.loc[zeroleak, c]\n",
    "        leaky_value_counts.append(sum(test_leak[\"compiled_leak\"] > 0))\n",
    "        #_correct_counts = sum(train_leak[\"compiled_leak\"]==train_leak[\"target\"])\n",
    "        #leaky_value_corrects.append(_correct_counts/leaky_value_counts[-1])\n",
    "        print(\"Leak values found in test\", leaky_value_counts[-1])\n",
    "        #print(\n",
    "        #    \"% of correct leaks values in train \", \n",
    "        #    leaky_value_corrects[-1]\n",
    "        #)\n",
    "        #tmp = train_leak.copy()\n",
    "        #tmp.loc[zeroleak, \"compiled_leak\"] = tmp.loc[zeroleak, \"nonzero_mean\"]\n",
    "        #scores.append(np.sqrt(mean_squared_error(y, np.log1p(tmp[\"compiled_leak\"]).fillna(14.49))))\n",
    "        #print(\n",
    "        #    'Score (filled with nonzero mean)', \n",
    "        #    scores[-1]\n",
    "        #)\n",
    "    result = dict(\n",
    "        # score=scores, \n",
    "        leaky_count=leaky_value_counts,\n",
    "        # leaky_correct=leaky_value_corrects,\n",
    "    )\n",
    "    return test_leak, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "a85699f8864b93664abde5b893426f94583bb7ab",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_leak, test_result = compiled_leak_result_test(max_nlags=38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f581e7e9f6a68169f2897e44ba9e8919ac6241c6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_result = pd.DataFrame.from_dict(test_result, orient='columns')\n",
    "test_result.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3559543eccb2c35ae6f06a842f67f9aa54fa264d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_result.to_csv('test_leaky_stat.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "06015b64764b4998669bb7fef877bb781b3ebc1a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_leak = rewrite_compiled_leak(test_leak, best_lag)\n",
    "test_leak[['ID']+leaky_cols+['compiled_leak']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ebca69ea7200a5245e8feeab4cb0b20a2606fb27",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_res = test_leak[leaky_cols+['compiled_leak']].replace(0.0, np.nan)\n",
    "test_res.to_csv('test_leak.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3f92ce5d52d5d2676adba8412d8ca8ce9983761e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_leak.loc[test_leak[\"compiled_leak\"]==0, \"compiled_leak\"] = test_leak.loc[test_leak[\"compiled_leak\"]==0, \"nonzero_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dc2e522df90f97456e67f26977fde364acf02876",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#submission\n",
    "sub = test[[\"ID\"]]\n",
    "sub[\"target\"] = test_leak[\"compiled_leak\"]\n",
    "sub.to_csv(f\"baseline_sub_lag_{best_lag}.csv\", index=False)\n",
    "print(f\"baseline_sub_lag_{best_lag}.csv saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9f8fa3cbae8cb81a1911983877e2a9aeda9c3bff",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f958838c19da7ed87e9a3529cb5fce2205e46ff2",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
